# Dmitry CLI

A Node.js terminal interface with local model support via Genkit framework and Ollama.

## Features

- ðŸš€ React-based terminal UI using Ink framework
- ðŸ’¬ Chat with local models through Ollama
- ðŸ”„ Easy model switching with slash commands
- âš¡ Streaming responses (simulated)
- ðŸŽ¨ Colorful and interactive interface

## Prerequisites

1. **Node.js v20+** - Required for running the CLI
2. **Ollama** - Must be installed and running locally
   ```bash
   # Install Ollama from https://ollama.ai
   # Start Ollama service
   ollama serve
   
   # Pull some models
   ollama pull llama3.2
   ollama pull gemma2
   ```

## Installation

```bash
# Install dependencies
npm install

# Test Ollama connection (optional)
node test-ollama.js

# Run the CLI
npm start
```

## Usage

### Basic Chat
Simply type your message and press Enter to chat with the model.

### Slash Commands

- `/help` - Show available commands
- `/model` - Show current model and list available models
- `/model <name>` - Switch to a different model (e.g., `/model gemma2`)

### Keyboard Shortcuts

- `Enter` - Send message
- `Backspace/Delete` - Delete characters
- `Left/Right Arrow` - Move cursor
- `Ctrl+C` - Exit the application

## Architecture

The project is structured as a monorepo with two main packages:

### `packages/cli`
- Terminal UI using Ink (React for terminals)
- Input handling and display
- Slash command processing

### `packages/core`
- Genkit integration for model communication
- Session management
- Provider abstraction for Ollama

## Configuration

The Genkit provider is configured in `packages/core/src/providers/GenkitProvider.js`:

```javascript
{
  models: [
    { name: 'llama3.2', type: 'generate' },
    { name: 'gemma2', type: 'generate' },
    { name: 'mistral', type: 'generate' },
    { name: 'llama3.1', type: 'generate' }
  ],
  serverAddress: 'http://127.0.0.1:11434' // Ollama default
}
```

## Troubleshooting

### "Cannot connect to Ollama"
Make sure Ollama is running:
```bash
ollama serve
```

### Model not available
Pull the model first:
```bash
ollama pull <model-name>
```

### Build errors
Make sure you're using Node.js v20+:
```bash
node --version
```

## Development

### Project Structure
```
dmitry-cli/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ cli/           # Terminal UI
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ App.js
â”‚   â”‚   â”‚   â”œâ”€â”€ index.js
â”‚   â”‚   â”‚   â””â”€â”€ hooks/
â”‚   â”‚   â”‚       â””â”€â”€ useModelStream.js
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â””â”€â”€ core/          # Core logic
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ index.js
â”‚       â”‚   â””â”€â”€ providers/
â”‚       â”‚       â””â”€â”€ GenkitProvider.js
â”‚       â””â”€â”€ package.json
â”œâ”€â”€ package.json       # Root package.json
â””â”€â”€ README.md
```

### Adding New Models

1. Ensure the model is available in Ollama:
   ```bash
   ollama pull <new-model>
   ```

2. Update the model list in `useModelStream.js`:
   ```javascript
   models: [
     // ... existing models
     { name: 'new-model', type: 'generate' }
   ]
   ```

## Future Enhancements

- [ ] Real streaming support (when Genkit adds it)
- [ ] Model-specific parameters (temperature, max tokens)
- [ ] Conversation history persistence
- [ ] Export conversations
- [ ] Multi-turn context management
- [ ] Tool/function calling support

## License

MIT